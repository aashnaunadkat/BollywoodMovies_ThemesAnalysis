{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e91acdc-c9bc-43ad-a8df-8ebe9d3550db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GET POSTERS FOR MOVIES FROM POSTER LINKS\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load your movie CSV\n",
    "df = pd.read_csv(\"data/sample_100_movies.csv\")\n",
    "\n",
    "# Create posters directory if it doesn't exist\n",
    "poster_dir = \"data/posters\"\n",
    "os.makedirs(poster_dir, exist_ok=True)\n",
    "\n",
    "# Set user agent as required by Wikimedia\n",
    "headers = {\n",
    "    \"User-Agent\": \"BollywoodThemesProject/0.1 (github link; contact: contact_email)\"\n",
    "}\n",
    "\n",
    "# Initialize list to track failures\n",
    "failed_downloads = []\n",
    "\n",
    "# Loop through each movie\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Downloading posters\"):\n",
    "    imdb_id = row[\"imdb_id\"]\n",
    "    poster_url = row[\"poster_path\"]\n",
    "\n",
    "    if pd.isna(poster_url) or poster_url == \"N/A\":\n",
    "        failed_downloads.append(f\"{imdb_id}: Missing or 'N/A' poster URL\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        response = requests.get(poster_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        file_path = os.path.join(poster_dir, f\"{imdb_id}.jpg\")\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "    except Exception as e:\n",
    "        failed_downloads.append(f\"{imdb_id}: {e}\")\n",
    "\n",
    "# Write all failed downloads to a file\n",
    "if failed_downloads:\n",
    "    with open(\"../data/supplementary files/failed_poster_downloads.txt\", \"w\") as f:\n",
    "        for error in failed_downloads:\n",
    "            f.write(error + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8c412f-13e0-40bf-b073-e37905eb9f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GET REMAINING POSERS FORM WIKI LINKS\n",
    "import pandas as pd\n",
    "import requests\n",
    "import wikipedia\n",
    "import re\n",
    "import os\n",
    "\n",
    "# --- Step 1: Load IMDb â†’ Wikipedia link map ---\n",
    "df = pd.read_csv(\"../data/sample_100_movies.csv\")\n",
    "id_to_wiki = dict(zip(df[\"imdb_id\"], df[\"wiki_link\"]))\n",
    "\n",
    "# --- Step 2: Load failure file ---\n",
    "failure_file = \"../data/supplementary files/failed_poster_downloads.txt\"\n",
    "with open(failure_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    original_lines = [line.strip() for line in f if line.strip()]\n",
    "    failed_ids = [line.split(\":\")[0] for line in original_lines]\n",
    "\n",
    "# --- Step 3: Prepare output folder ---\n",
    "poster_dir = \"../data/posters\"\n",
    "os.makedirs(poster_dir, exist_ok=True)\n",
    "\n",
    "def get_first_image_url(wiki_url):\n",
    "    \"\"\"\n",
    "    Return the first valid Wikipedia poster image URL without BeautifulSoup.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        title = wiki_url.split(\"/wiki/\")[-1]\n",
    "        page_url = f\"https://en.wikipedia.org/wiki/{title}\"\n",
    "        res = requests.get(page_url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=10)\n",
    "        if res.status_code != 200:\n",
    "            return None\n",
    "\n",
    "        # Use regex to find all image tags\n",
    "        matches = re.findall(r'<img[^>]+src=\"([^\"]+)\"[^>]*>', res.text)\n",
    "        for img in matches:\n",
    "            if \"thumb\" in img and img.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                if img.startswith(\"//\"):\n",
    "                    return \"https:\" + img\n",
    "                elif img.startswith(\"/\"):\n",
    "                    return \"https://en.wikipedia.org\" + img\n",
    "                else:\n",
    "                    return img\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# --- Step 4: Attempt to download images ---\n",
    "successful_ids = []\n",
    "\n",
    "for imdb_id in failed_ids:\n",
    "    wiki_url = id_to_wiki.get(imdb_id)\n",
    "    if not wiki_url:\n",
    "        print(f\"No wiki link for {imdb_id}\")\n",
    "        continue\n",
    "\n",
    "    img_url = get_first_image_url(wiki_url)\n",
    "    if not img_url:\n",
    "        print(f\"No image found for {imdb_id}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        img_data = requests.get(img_url, headers={\"User-Agent\": \"Mozilla/5.0\"}).content\n",
    "        with open(f\"{poster_dir}/{imdb_id}.jpg\", \"wb\") as f:\n",
    "            f.write(img_data)\n",
    "        successful_ids.append(imdb_id)\n",
    "        print(f\"Recovered and saved poster for {imdb_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Download failed for {imdb_id}: {e}\")\n",
    "\n",
    "# --- Step 5: Rewrite the failure file with only unsuccessful records ---\n",
    "remaining_lines = [line for line in original_lines if line.split(\":\")[0] not in successful_ids]\n",
    "\n",
    "with open(failure_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in remaining_lines:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(f\"\\nCleaned failure file. {len(successful_ids)} posters recovered and removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41207164-4f6b-4705-86e1-e158c731fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GET SUBTITLES FOR MOVIES\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define variables with API key and login information to send OpenSubtitles API a login request and get a token\n",
    "API_KEY = \"my_opensubtitles_api_key\"\n",
    "USERNAME = \"my_opensubtitles_username\"\n",
    "PASSWORD = \"my_opensubtitles_password\"\n",
    "\n",
    "headers = {\n",
    "    \"Api-Key\": API_KEY,\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"User-Agent\": \"BollywoodThemesProject v0.1\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"username\": USERNAME,\n",
    "    \"password\": PASSWORD\n",
    "}\n",
    "\n",
    "response = requests.post(\"https://api.opensubtitles.com/api/v1/login\", headers=headers, json=data)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    token = response.json()[\"token\"]\n",
    "    print(\"New token:\", token)\n",
    "\n",
    "    # Save token to file for now, until the task is done\n",
    "    with open(\"opensubtitles_token.txt\", \"w\") as f:\n",
    "        f.write(token)\n",
    "else:\n",
    "    print(\"Login failed:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad1881a-3681-4a09-90b1-b5cc3e8f2910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your API values\n",
    "API_KEY = \"my_opensubtitles_api_key\"\n",
    "TOKEN = \"my_opensubtitles_token\"\n",
    "USER_AGENT = \"BollywoodThemesProject v0.1\" # user agent created for this project\n",
    "\n",
    "# Create folder to save subtitle files, if doesn't exist already\n",
    "output_folder = \"../data/subtitles/srt\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Load your CSV\n",
    "df = pd.read_csv(\"../data/sample_100_movies.csv\")\n",
    "imdb_ids = df[\"imdb_id\"].dropna().unique().tolist()\n",
    "\n",
    "# Reusable header\n",
    "HEADERS = {\n",
    "    \"Api-Key\": API_KEY,\n",
    "    \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "    \"User-Agent\": USER_AGENT,\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5f5f2a-ea99-4df1-a155-0f0229f68f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper function with retry ---\n",
    "def safe_request(func, *args, **kwargs):\n",
    "    try:\n",
    "        response = func(*args, **kwargs)\n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"{e}\")\n",
    "        return None\n",
    "\n",
    "# --- Download subtitle ---\n",
    "def download_subtitle(imdb_id):\n",
    "    numeric_id = imdb_id.replace(\"tt\", \"\")\n",
    "    \n",
    "    # Step 1: Search subtitles\n",
    "    search_url = \"https://api.opensubtitles.com/api/v1/subtitles\"\n",
    "    search_params = {\n",
    "        \"imdb_id\": numeric_id,\n",
    "        \"languages\": \"en\",\n",
    "        \"order_by\": \"downloads\",\n",
    "        \"order_direction\": \"desc\"\n",
    "    }\n",
    "    response = safe_request(requests.get, search_url, headers=HEADERS, params=search_params)\n",
    "    if not response or not response.json().get(\"data\"):\n",
    "        print(f\"No subtitles found for {imdb_id}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        file_id = response.json()[\"data\"][0][\"attributes\"][\"files\"][0][\"file_id\"]\n",
    "    except Exception:\n",
    "        print(f\"file_id missing for {imdb_id}\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Get download link\n",
    "    download_url = \"https://api.opensubtitles.com/api/v1/download\"\n",
    "    download_payload = { \"file_id\": file_id }\n",
    "    download_response = safe_request(requests.post, download_url, headers=HEADERS, json=download_payload)\n",
    "    if not download_response or not download_response.json().get(\"link\"):\n",
    "        print(f\"Download link failed for {imdb_id}\")\n",
    "        return\n",
    "    \n",
    "    subtitle_url = download_response.json()[\"link\"]\n",
    "\n",
    "    # Step 3: Download subtitle file\n",
    "    subtitle_file = safe_request(requests.get, subtitle_url)\n",
    "    if not subtitle_file:\n",
    "        print(f\"Failed to download subtitle for {imdb_id}\")\n",
    "        return\n",
    "    \n",
    "    filepath = os.path.join(output_folder, f\"{imdb_id}.srt\")\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        f.write(subtitle_file.content)\n",
    "    \n",
    "    print(f\"Downloaded subtitle for {imdb_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426fff0e-456f-4bb2-a125-03fb325e1f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Loop ---\n",
    "for imdb_id in tqdm(imdb_ids, desc=\"Downloading subtitles\"):\n",
    "    download_subtitle(imdb_id)\n",
    "    time.sleep(10)  # Respect API rate limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5975e1ff-9909-42f7-8067-74e74edd8e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GET DESCRIPTIONS FOR MOVIES using Cinemagoer library which scrapes IMDb\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import imdb\n",
    "ia = imdb.Cinemagoer()\n",
    "\n",
    "# Load your CSV file\n",
    "df = pd.read_csv(\"../data/sample_100_movies.csv\")  # Update path if needed\n",
    "\n",
    "# Extract the imdb_ids column as a list\n",
    "imdb_ids = df['imdb_id'].dropna().unique().tolist()\n",
    "\n",
    "# Define the output folder\n",
    "output_folder = \"../data/descriptions\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Tracking missing entries\n",
    "missing_count = 0\n",
    "\n",
    "# Loop through IMDb IDs\n",
    "for imdb_id in imdb_ids:\n",
    "    imdb_id_clean = imdb_id.replace(\"tt\", \"\")  # Cinemagoer uses numeric ID only\n",
    "\n",
    "    try:\n",
    "        # Fetch movie data\n",
    "        movie = ia.get_movie(imdb_id_clean, info=['synopsis', 'plot'])\n",
    "\n",
    "        # Try to extract synopsis\n",
    "        synopsis_list = movie.get('synopsis', [])\n",
    "\n",
    "        if synopsis_list:\n",
    "            synopsis = synopsis_list[0].split(\"::\")[0].strip()\n",
    "            file_name = f\"{imdb_id}.txt\"\n",
    "        else:\n",
    "            # Fallback to plot if no synopsis\n",
    "            plot_list = movie.get('plot', [])\n",
    "            if plot_list:\n",
    "                synopsis = plot_list[0].split(\"::\")[0].strip()\n",
    "                file_name = f\"{imdb_id}.txt\"\n",
    "            else:\n",
    "                # Neither synopsis nor plot found\n",
    "                synopsis = \"No synopsis or plot available.\"\n",
    "                file_name = f\"FAILED-{imdb_id}.txt\"\n",
    "                missing_count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        # Error fetching data from IMDb\n",
    "        synopsis = f\"Error retrieving synopsis: {e}\"\n",
    "        file_name = f\"FAILED-{imdb_id}.txt\"\n",
    "        missing_count += 1\n",
    "\n",
    "    # Save synopsis or error message to file\n",
    "    output_path = os.path.join(output_folder, file_name)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(synopsis)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nFinished processing all movies.\")\n",
    "print(f\"Could not retrieve synopsis/plot for {missing_count} movie(s). Files named with 'FAILED-'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e801fb3a-9b79-497f-9043-347346837b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GET DESCRIPTIONS FOR REMAINING MOVIES FROM WIKIPEDIA\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Load the full movie dataset\n",
    "df = pd.read_csv(\"../data/sample_100_movies.csv\")\n",
    "\n",
    "# Ensure output folder exists\n",
    "output_folder = \"../data/descriptions\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Plot-related section headers to search for\n",
    "PLOT_KEYWORDS = [\"plot\", \"synopsis\", \"plot summary\", \"plot synopsis\"]\n",
    "\n",
    "# Track success/failure\n",
    "success_count = 0\n",
    "failure_count = 0\n",
    "failed_ids = []\n",
    "\n",
    "def get_canonical_page_id(url):\n",
    "    \"\"\"Extract reliable page ID by resolving the canonical title using redirects.\"\"\"\n",
    "    title = url.strip().rsplit('/', 1)[-1]\n",
    "    api_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'titles': title,\n",
    "        'redirects': 1\n",
    "    }\n",
    "    response = requests.get(api_url, params=params)\n",
    "    data = response.json()\n",
    "    pages = data['query']['pages']\n",
    "    page_id = next(iter(pages))\n",
    "    return int(page_id) if page_id != \"-1\" else None\n",
    "\n",
    "def get_page_content_by_id(page_id):\n",
    "    \"\"\"Fetch full plain-text content of a Wikipedia page by its ID.\"\"\"\n",
    "    api_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'prop': 'extracts',\n",
    "        'explaintext': True,\n",
    "        'pageids': page_id\n",
    "    }\n",
    "    response = requests.get(api_url, params=params)\n",
    "    data = response.json()\n",
    "    return data['query']['pages'][str(page_id)]['extract']\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    imdb_id = row.get(\"imdb_id\")\n",
    "    wiki_url = row.get(\"wiki_link\")\n",
    "\n",
    "    if pd.isna(imdb_id) or pd.isna(wiki_url):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Step 1: Get canonical Page ID\n",
    "        page_id = get_canonical_page_id(wiki_url)\n",
    "        if not page_id:\n",
    "            print(f\"IMDb ID {imdb_id} â€” Wikipedia page ID not found.\")\n",
    "            failure_count += 1\n",
    "            failed_ids.append(imdb_id)\n",
    "            continue\n",
    "\n",
    "        # Step 2: Get content using page ID\n",
    "        content = get_page_content_by_id(page_id)\n",
    "\n",
    "        # Step 3: Look for plot/synopsis section\n",
    "        sections = re.split(r'\\n==+ *(.+?) *==+\\n', content)\n",
    "        plot_text = None\n",
    "        for i in range(1, len(sections), 2):\n",
    "            title = sections[i].strip().lower()\n",
    "            body = sections[i + 1].strip()\n",
    "            if any(k in title for k in PLOT_KEYWORDS):\n",
    "                plot_text = body\n",
    "                break\n",
    "\n",
    "        if plot_text:\n",
    "            file_path = os.path.join(output_folder, f\"{imdb_id}.txt\")\n",
    "            mode = \"a\" if os.path.exists(file_path) else \"w\"\n",
    "            with open(file_path, mode, encoding=\"utf-8\") as f:\n",
    "                f.write(\"\\n\\n[Wikipedia Plot Synopsis]\\n\")\n",
    "                f.write(plot_text)\n",
    "            success_count += 1\n",
    "        else:\n",
    "            print(f\"IMDb ID {imdb_id} â€” Plot section not found.\")\n",
    "            failure_count += 1\n",
    "            failed_ids.append(imdb_id)\n",
    "\n",
    "        time.sleep(0.5)  # Be kind to Wikipedia's servers\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"IMDb ID {imdb_id} â€” Error: {e}\")\n",
    "        failure_count += 1\n",
    "        failed_ids.append(imdb_id)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nWikipedia extraction complete.\")\n",
    "print(f\"Successfully retrieved plot for {success_count} movies.\")\n",
    "print(f\"Failed or missing for {failure_count} movies.\")\n",
    "\n",
    "# Save failed IDs\n",
    "if failed_ids:\n",
    "    with open(\"../data/supplementary files/failed_wikipedia_plots.txt\", \"w\") as f:\n",
    "        for fid in failed_ids:\n",
    "            f.write(fid + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
