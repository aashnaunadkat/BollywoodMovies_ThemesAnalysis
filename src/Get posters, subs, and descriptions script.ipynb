{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97bcae0b-d0cf-41a6-b9c7-1f7044ae6c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading posters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:27<00:00,  3.61it/s]\n"
     ]
    }
   ],
   "source": [
    "## GET POSTERS FOR MOVIES FROM POSTER LINKS\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load your movie CSV\n",
    "df = pd.read_csv(\"data/sample_100_movies.csv\")\n",
    "\n",
    "# Create posters directory if it doesn't exist\n",
    "poster_dir = \"data/posters\"\n",
    "os.makedirs(poster_dir, exist_ok=True)\n",
    "\n",
    "# Set user agent as required by Wikimedia\n",
    "headers = {\n",
    "    \"User-Agent\": \"BollywoodThemesProject/0.1 (github link; contact: contact_email)\"\n",
    "}\n",
    "\n",
    "# Initialize list to track failures\n",
    "failed_downloads = []\n",
    "\n",
    "# Loop through each movie\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Downloading posters\"):\n",
    "    imdb_id = row[\"imdb_id\"]\n",
    "    poster_url = row[\"poster_path\"]\n",
    "\n",
    "    if pd.isna(poster_url) or poster_url == \"N/A\":\n",
    "        failed_downloads.append(f\"{imdb_id}: Missing or 'N/A' poster URL\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        response = requests.get(poster_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        file_path = os.path.join(poster_dir, f\"{imdb_id}.jpg\")\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "    except Exception as e:\n",
    "        failed_downloads.append(f\"{imdb_id}: {e}\")\n",
    "\n",
    "# Write all failed downloads to a file\n",
    "if failed_downloads:\n",
    "    with open(\"../data/supplementary files/failed_poster_downloads.txt\", \"w\") as f:\n",
    "        for error in failed_downloads:\n",
    "            f.write(error + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a1c2fc4-1f49-4243-bb0d-a8e0123c16ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Recovered and saved poster for tt7743400\n",
      "‚úÖ Recovered and saved poster for tt3893476\n",
      "‚úÖ Recovered and saved poster for tt4853926\n",
      "‚úÖ Recovered and saved poster for tt3017412\n",
      "‚ùå No image found for tt1353093\n",
      "‚úÖ Recovered and saved poster for tt9558612\n",
      "‚úÖ Recovered and saved poster for tt5668770\n",
      "\n",
      "üßπ Cleaned failure file. 6 posters recovered and removed.\n"
     ]
    }
   ],
   "source": [
    "## GET REMAINING POSERS FORM WIKI LINKS\n",
    "import pandas as pd\n",
    "import requests\n",
    "import wikipedia\n",
    "import re\n",
    "import os\n",
    "\n",
    "# --- Step 1: Load IMDb ‚Üí Wikipedia link map ---\n",
    "df = pd.read_csv(\"../data/sample_100_movies.csv\")\n",
    "id_to_wiki = dict(zip(df[\"imdb_id\"], df[\"wiki_link\"]))\n",
    "\n",
    "# --- Step 2: Load failure file ---\n",
    "failure_file = \"../data/supplementary files/failed_poster_downloads.txt\"\n",
    "with open(failure_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    original_lines = [line.strip() for line in f if line.strip()]\n",
    "    failed_ids = [line.split(\":\")[0] for line in original_lines]\n",
    "\n",
    "# --- Step 3: Prepare output folder ---\n",
    "poster_dir = \"../data/posters\"\n",
    "os.makedirs(poster_dir, exist_ok=True)\n",
    "\n",
    "def get_first_image_url(wiki_url):\n",
    "    \"\"\"\n",
    "    Return the first valid Wikipedia poster image URL without BeautifulSoup.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        title = wiki_url.split(\"/wiki/\")[-1]\n",
    "        page_url = f\"https://en.wikipedia.org/wiki/{title}\"\n",
    "        res = requests.get(page_url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=10)\n",
    "        if res.status_code != 200:\n",
    "            return None\n",
    "\n",
    "        # Use regex to find all image tags\n",
    "        matches = re.findall(r'<img[^>]+src=\"([^\"]+)\"[^>]*>', res.text)\n",
    "        for img in matches:\n",
    "            if \"thumb\" in img and img.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                if img.startswith(\"//\"):\n",
    "                    return \"https:\" + img\n",
    "                elif img.startswith(\"/\"):\n",
    "                    return \"https://en.wikipedia.org\" + img\n",
    "                else:\n",
    "                    return img\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# --- Step 4: Attempt to download images ---\n",
    "successful_ids = []\n",
    "\n",
    "for imdb_id in failed_ids:\n",
    "    wiki_url = id_to_wiki.get(imdb_id)\n",
    "    if not wiki_url:\n",
    "        print(f\"No wiki link for {imdb_id}\")\n",
    "        continue\n",
    "\n",
    "    img_url = get_first_image_url(wiki_url)\n",
    "    if not img_url:\n",
    "        print(f\"‚ùå No image found for {imdb_id}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        img_data = requests.get(img_url, headers={\"User-Agent\": \"Mozilla/5.0\"}).content\n",
    "        with open(f\"{poster_dir}/{imdb_id}.jpg\", \"wb\") as f:\n",
    "            f.write(img_data)\n",
    "        successful_ids.append(imdb_id)\n",
    "        print(f\"‚úÖ Recovered and saved poster for {imdb_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Download failed for {imdb_id}: {e}\")\n",
    "\n",
    "# --- Step 5: Rewrite the failure file with only unsuccessful records ---\n",
    "remaining_lines = [line for line in original_lines if line.split(\":\")[0] not in successful_ids]\n",
    "\n",
    "with open(failure_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in remaining_lines:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(f\"\\nüßπ Cleaned failure file. {len(successful_ids)} posters recovered and removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17b02b8e-c041-41fe-8bce-54cc18d2d8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ New token: eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiI2RXFqVDRTME14STllRnJxQzI5UE9HWVJ2R2lrTDUwMiIsImV4cCI6MTc1MzM0MDgzNX0.Dj5YzDqIkJSP54SwGNvoU-qm9V1vnv9gYgBcezUwgq0\n"
     ]
    }
   ],
   "source": [
    "## GET SUBTITLES FOR MOVIES\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define variables with API key and login information to send OpenSubtitles API a login request and get a token\n",
    "API_KEY = \"my_opensubtitles_api_key\"\n",
    "USERNAME = \"my_opensubtitles_username\"\n",
    "PASSWORD = \"my_opensubtitles_password\"\n",
    "\n",
    "headers = {\n",
    "    \"Api-Key\": API_KEY,\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"User-Agent\": \"BollywoodThemesProject v0.1\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"username\": USERNAME,\n",
    "    \"password\": PASSWORD\n",
    "}\n",
    "\n",
    "response = requests.post(\"https://api.opensubtitles.com/api/v1/login\", headers=headers, json=data)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    token = response.json()[\"token\"]\n",
    "    print(\"New token:\", token)\n",
    "\n",
    "    # Save token to file for now, until the task is done\n",
    "    with open(\"opensubtitles_token.txt\", \"w\") as f:\n",
    "        f.write(token)\n",
    "else:\n",
    "    print(\"Login failed:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5bc64fb7-cb90-4c4a-8206-84e718b1e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your API values\n",
    "API_KEY = \"my_opensubtitles_api_key\"\n",
    "TOKEN = \"my_opensubtitles_token\"\n",
    "USER_AGENT = \"BollywoodThemesProject v0.1\" # user agent created for this project\n",
    "\n",
    "# Create folder to save subtitle files, if doesn't exist already\n",
    "output_folder = \"../data/subtitles/srt\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Load your CSV\n",
    "df = pd.read_csv(\"../data/sample_100_movies.csv\")\n",
    "imdb_ids = df[\"imdb_id\"].dropna().unique().tolist()\n",
    "\n",
    "# Reusable header\n",
    "HEADERS = {\n",
    "    \"Api-Key\": API_KEY,\n",
    "    \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "    \"User-Agent\": USER_AGENT,\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dc4926f-afa9-4574-bcec-fa608d88396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper function with retry ---\n",
    "def safe_request(func, *args, **kwargs):\n",
    "    try:\n",
    "        response = func(*args, **kwargs)\n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"{e}\")\n",
    "        return None\n",
    "\n",
    "# --- Download subtitle ---\n",
    "def download_subtitle(imdb_id):\n",
    "    numeric_id = imdb_id.replace(\"tt\", \"\")\n",
    "    \n",
    "    # Step 1: Search subtitles\n",
    "    search_url = \"https://api.opensubtitles.com/api/v1/subtitles\"\n",
    "    search_params = {\n",
    "        \"imdb_id\": numeric_id,\n",
    "        \"languages\": \"en\",\n",
    "        \"order_by\": \"downloads\",\n",
    "        \"order_direction\": \"desc\"\n",
    "    }\n",
    "    response = safe_request(requests.get, search_url, headers=HEADERS, params=search_params)\n",
    "    if not response or not response.json().get(\"data\"):\n",
    "        print(f\"No subtitles found for {imdb_id}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        file_id = response.json()[\"data\"][0][\"attributes\"][\"files\"][0][\"file_id\"]\n",
    "    except Exception:\n",
    "        print(f\"file_id missing for {imdb_id}\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Get download link\n",
    "    download_url = \"https://api.opensubtitles.com/api/v1/download\"\n",
    "    download_payload = { \"file_id\": file_id }\n",
    "    download_response = safe_request(requests.post, download_url, headers=HEADERS, json=download_payload)\n",
    "    if not download_response or not download_response.json().get(\"link\"):\n",
    "        print(f\"Download link failed for {imdb_id}\")\n",
    "        return\n",
    "    \n",
    "    subtitle_url = download_response.json()[\"link\"]\n",
    "\n",
    "    # Step 3: Download subtitle file\n",
    "    subtitle_file = safe_request(requests.get, subtitle_url)\n",
    "    if not subtitle_file:\n",
    "        print(f\"Failed to download subtitle for {imdb_id}\")\n",
    "        return\n",
    "    \n",
    "    filepath = os.path.join(output_folder, f\"{imdb_id}.srt\")\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        f.write(subtitle_file.content)\n",
    "    \n",
    "    print(f\"Downloaded subtitle for {imdb_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0cffcca7-ba86-4774-b636-bec012c8a2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading subtitles:   0%|                                                                    | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No subtitles found for tt2554042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading subtitles:   4%|‚ñà‚ñà‚ñå                                                         | 1/23 [00:10<03:56, 10.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No subtitles found for tt7260848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading subtitles:   9%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                      | 2/23 [00:21<03:42, 10.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No subtitles found for tt6040012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading subtitles:  13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                    | 3/23 [00:32<03:34, 10.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No subtitles found for tt7743400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading subtitles:  17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                 | 4/23 [00:42<03:23, 10.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No subtitles found for tt3893476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading subtitles:  22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                               | 5/23 [00:53<03:11, 10.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No subtitles found for tt2294685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading subtitles:  26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                            | 6/23 [01:03<02:59, 10.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No subtitles found for tt6862542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading subtitles:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                         | 7/23 [01:14<02:48, 10.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No subtitles found for tt4853926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading subtitles:  35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                       | 8/23 [01:24<02:37, 10.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "‚ùå No subtitles found for tt2387495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading subtitles:  39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                    | 9/23 [01:35<02:26, 10.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No subtitles found for tt4493550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading subtitles:  43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                 | 10/23 [01:45<02:15, 10.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "‚ùå No subtitles found for tt4249442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading subtitles:  48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 11/23 [01:55<02:04, 10.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No subtitles found for tt3021244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading subtitles:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                            | 12/23 [02:06<01:54, 10.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No subtitles found for tt1942905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading subtitles:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 13/23 [02:16<01:43, 10.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No subtitles found for tt4335698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading subtitles:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 14/23 [02:26<01:33, 10.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "‚ùå No subtitles found for tt1353093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading subtitles:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 15/23 [02:37<01:22, 10.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "‚ùå No subtitles found for tt6118134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading subtitles:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 16/23 [02:47<01:12, 10.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No subtitles found for tt4581032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading subtitles:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 17/23 [02:57<01:02, 10.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No subtitles found for tt5933706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading subtitles:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 18/23 [03:08<00:51, 10.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "‚ùå No subtitles found for tt2404187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading subtitles:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 19/23 [03:18<00:41, 10.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No subtitles found for tt4995402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading subtitles:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 20/23 [03:28<00:31, 10.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No subtitles found for tt5752374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading subtitles:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 21/23 [03:39<00:20, 10.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "‚ùå No subtitles found for tt2150177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading subtitles:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 22/23 [03:49<00:10, 10.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No subtitles found for tt4340180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading subtitles: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23/23 [04:00<00:00, 10.44s/it]\n"
     ]
    }
   ],
   "source": [
    "# --- Main Loop ---\n",
    "for imdb_id in tqdm(imdb_ids, desc=\"Downloading subtitles\"):\n",
    "    download_subtitle(imdb_id)\n",
    "    time.sleep(10)  # Respect API rate limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca80230d-e0da-494f-a870-abe76a03d62a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f389927f-75c4-45ae-bf4a-f00c25ee3ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Finished processing all movies.\n",
      "‚ö†Ô∏è Could not retrieve synopsis/plot for 0 movie(s). Files named with 'FAILED-'.\n"
     ]
    }
   ],
   "source": [
    "## GET DESCRIPTIONS FOR MOVIES using Cinemagoer library which scrapes IMDb\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import imdb\n",
    "ia = imdb.Cinemagoer()\n",
    "\n",
    "# Load your CSV file\n",
    "df = pd.read_csv(\"../data/sample_100_movies.csv\")  # Update path if needed\n",
    "\n",
    "# Extract the imdb_ids column as a list\n",
    "imdb_ids = df['imdb_id'].dropna().unique().tolist()\n",
    "\n",
    "# Define the output folder\n",
    "output_folder = \"../data/descriptions\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Tracking missing entries\n",
    "missing_count = 0\n",
    "\n",
    "# Loop through IMDb IDs\n",
    "for imdb_id in imdb_ids:\n",
    "    imdb_id_clean = imdb_id.replace(\"tt\", \"\")  # Cinemagoer uses numeric ID only\n",
    "\n",
    "    try:\n",
    "        # Fetch movie data\n",
    "        movie = ia.get_movie(imdb_id_clean, info=['synopsis', 'plot'])\n",
    "\n",
    "        # Try to extract synopsis\n",
    "        synopsis_list = movie.get('synopsis', [])\n",
    "\n",
    "        if synopsis_list:\n",
    "            synopsis = synopsis_list[0].split(\"::\")[0].strip()\n",
    "            file_name = f\"{imdb_id}.txt\"\n",
    "        else:\n",
    "            # Fallback to plot if no synopsis\n",
    "            plot_list = movie.get('plot', [])\n",
    "            if plot_list:\n",
    "                synopsis = plot_list[0].split(\"::\")[0].strip()\n",
    "                file_name = f\"{imdb_id}.txt\"\n",
    "            else:\n",
    "                # Neither synopsis nor plot found\n",
    "                synopsis = \"No synopsis or plot available.\"\n",
    "                file_name = f\"FAILED-{imdb_id}.txt\"\n",
    "                missing_count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        # Error fetching data from IMDb\n",
    "        synopsis = f\"Error retrieving synopsis: {e}\"\n",
    "        file_name = f\"FAILED-{imdb_id}.txt\"\n",
    "        missing_count += 1\n",
    "\n",
    "    # Save synopsis or error message to file\n",
    "    output_path = os.path.join(output_folder, file_name)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(synopsis)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n‚úÖ Finished processing all movies.\")\n",
    "print(f\"‚ö†Ô∏è Could not retrieve synopsis/plot for {missing_count} movie(s). Files named with 'FAILED-'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bccef63b-6c90-45ad-9305-3e5b41fab623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\aashna unadkat\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from wikipedia) (4.13.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\aashna unadkat\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from wikipedia) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aashna unadkat\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aashna unadkat\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aashna unadkat\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aashna unadkat\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.7.14)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\aashna unadkat\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\aashna unadkat\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4->wikipedia) (4.14.1)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (pyproject.toml): started\n",
      "  Building wheel for wikipedia (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11785 sha256=af57682b6b78ffcb4816e7b05afbbf233c53df8033ecdf5594930f67fb73ec8e\n",
      "  Stored in directory: c:\\users\\aashna unadkat\\appdata\\local\\pip\\cache\\wheels\\79\\1d\\c8\\b64e19423cc5a2a339450ea5d145e7c8eb3d4aa2b150cde33b\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36da85fc-cd85-458d-88c8-1c682a3d9526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è IMDb ID tt3893476 ‚Äî Plot section not found.\n",
      "‚ö†Ô∏è IMDb ID tt6862542 ‚Äî Plot section not found.\n",
      "‚ö†Ô∏è IMDb ID tt5615116 ‚Äî Plot section not found.\n",
      "‚ö†Ô∏è IMDb ID tt4744086 ‚Äî Wikipedia page ID not found.\n",
      "‚ö†Ô∏è IMDb ID tt7260848 ‚Äî Plot section not found.\n",
      "‚ö†Ô∏è IMDb ID tt3017412 ‚Äî Plot section not found.\n",
      "‚ö†Ô∏è IMDb ID tt4249442 ‚Äî Plot section not found.\n",
      "‚ö†Ô∏è IMDb ID tt4335698 ‚Äî Plot section not found.\n",
      "‚ö†Ô∏è IMDb ID tt1353093 ‚Äî Wikipedia page ID not found.\n",
      "‚ö†Ô∏è IMDb ID tt8484942 ‚Äî Wikipedia page ID not found.\n",
      "‚ö†Ô∏è IMDb ID tt6118134 ‚Äî Plot section not found.\n",
      "‚ö†Ô∏è IMDb ID tt4340180 ‚Äî Plot section not found.\n",
      "\n",
      "‚úÖ Wikipedia extraction complete.\n",
      "üü¢ Successfully retrieved plot for 89 movies.\n",
      "üî¥ Failed or missing for 12 movies.\n"
     ]
    }
   ],
   "source": [
    "## GET DESCRIPTIONS FOR REMAINING MOVIES FROM WIKIPEDIA\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Load the full movie dataset\n",
    "df = pd.read_csv(\"../data/sample_100_movies.csv\")\n",
    "\n",
    "# Ensure output folder exists\n",
    "output_folder = \"../data/descriptions\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Plot-related section headers to search for\n",
    "PLOT_KEYWORDS = [\"plot\", \"synopsis\", \"plot summary\", \"plot synopsis\"]\n",
    "\n",
    "# Track success/failure\n",
    "success_count = 0\n",
    "failure_count = 0\n",
    "failed_ids = []\n",
    "\n",
    "def get_canonical_page_id(url):\n",
    "    \"\"\"Extract reliable page ID by resolving the canonical title using redirects.\"\"\"\n",
    "    title = url.strip().rsplit('/', 1)[-1]\n",
    "    api_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'titles': title,\n",
    "        'redirects': 1\n",
    "    }\n",
    "    response = requests.get(api_url, params=params)\n",
    "    data = response.json()\n",
    "    pages = data['query']['pages']\n",
    "    page_id = next(iter(pages))\n",
    "    return int(page_id) if page_id != \"-1\" else None\n",
    "\n",
    "def get_page_content_by_id(page_id):\n",
    "    \"\"\"Fetch full plain-text content of a Wikipedia page by its ID.\"\"\"\n",
    "    api_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'prop': 'extracts',\n",
    "        'explaintext': True,\n",
    "        'pageids': page_id\n",
    "    }\n",
    "    response = requests.get(api_url, params=params)\n",
    "    data = response.json()\n",
    "    return data['query']['pages'][str(page_id)]['extract']\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    imdb_id = row.get(\"imdb_id\")\n",
    "    wiki_url = row.get(\"wiki_link\")\n",
    "\n",
    "    if pd.isna(imdb_id) or pd.isna(wiki_url):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Step 1: Get canonical Page ID\n",
    "        page_id = get_canonical_page_id(wiki_url)\n",
    "        if not page_id:\n",
    "            print(f\"‚ö†Ô∏è IMDb ID {imdb_id} ‚Äî Wikipedia page ID not found.\")\n",
    "            failure_count += 1\n",
    "            failed_ids.append(imdb_id)\n",
    "            continue\n",
    "\n",
    "        # Step 2: Get content using page ID\n",
    "        content = get_page_content_by_id(page_id)\n",
    "\n",
    "        # Step 3: Look for plot/synopsis section\n",
    "        sections = re.split(r'\\n==+ *(.+?) *==+\\n', content)\n",
    "        plot_text = None\n",
    "        for i in range(1, len(sections), 2):\n",
    "            title = sections[i].strip().lower()\n",
    "            body = sections[i + 1].strip()\n",
    "            if any(k in title for k in PLOT_KEYWORDS):\n",
    "                plot_text = body\n",
    "                break\n",
    "\n",
    "        if plot_text:\n",
    "            file_path = os.path.join(output_folder, f\"{imdb_id}.txt\")\n",
    "            mode = \"a\" if os.path.exists(file_path) else \"w\"\n",
    "            with open(file_path, mode, encoding=\"utf-8\") as f:\n",
    "                f.write(\"\\n\\n[Wikipedia Plot Synopsis]\\n\")\n",
    "                f.write(plot_text)\n",
    "            success_count += 1\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è IMDb ID {imdb_id} ‚Äî Plot section not found.\")\n",
    "            failure_count += 1\n",
    "            failed_ids.append(imdb_id)\n",
    "\n",
    "        time.sleep(0.5)  # Be kind to Wikipedia's servers\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå IMDb ID {imdb_id} ‚Äî Error: {e}\")\n",
    "        failure_count += 1\n",
    "        failed_ids.append(imdb_id)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n‚úÖ Wikipedia extraction complete.\")\n",
    "print(f\"üü¢ Successfully retrieved plot for {success_count} movies.\")\n",
    "print(f\"üî¥ Failed or missing for {failure_count} movies.\")\n",
    "\n",
    "# Save failed IDs\n",
    "if failed_ids:\n",
    "    with open(\"../data/supplementary files/failed_wikipedia_plots.txt\", \"w\") as f:\n",
    "        for fid in failed_ids:\n",
    "            f.write(fid + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e801fb3a-9b79-497f-9043-347346837b57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
