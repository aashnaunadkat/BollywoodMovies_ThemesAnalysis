{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189e5d32-7bdb-41b6-893c-59f0c9012947",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DIRECTOR INFORMATION\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Load filtered movie dataset\n",
    "df_movies = pd.read_csv(\"../data/sample_100_movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50814f9a-c0a5-433b-b1b1-bebb39da8be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First add director names to dataset using OMDb API and IMDb ID.\n",
    "\n",
    "OMDB_API_KEY = \"my_omdb_api_key\"\n",
    "\n",
    "# Load your movie CSV (update path as needed)\n",
    "df = pd.read_csv(\"../data/sample_100_movies.csv\")\n",
    "\n",
    "# Initialize new columns\n",
    "df[\"director_1_name\"] = \"\"\n",
    "df[\"director_2_name\"] = \"\"\n",
    "\n",
    "# Function to get director names from OMDb\n",
    "def get_director_names(imdb_id):\n",
    "    url = f\"http://www.omdbapi.com/?i={imdb_id}&apikey={OMDB_API_KEY}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        directors = data.get(\"Director\", \"\")\n",
    "        if directors == \"N/A\":\n",
    "            return []\n",
    "        return [d.strip() for d in directors.split(\",\")]\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error fetching director for {imdb_id}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Loop through rows to fetch and populate names\n",
    "for idx, row in df.iterrows():\n",
    "    imdb_id = row[\"imdb_id\"]\n",
    "    director_names = get_director_names(imdb_id)\n",
    "    \n",
    "    if len(director_names) > 0:\n",
    "        df.at[idx, \"director_1_name\"] = director_names[0]\n",
    "    if len(director_names) > 1:\n",
    "        df.at[idx, \"director_2_name\"] = director_names[1]\n",
    "    \n",
    "    time.sleep(1)  # Be polite to the API\n",
    "\n",
    "# Save updated file\n",
    "df.to_csv(\"../data/sample_100_movies.csv\", index=False)\n",
    "print(\"Director names added and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c191f82f-6a10-460e-9375-8128e5a76c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try to infer gender from Wikipedia page of the director.\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def get_gender_from_wikipedia(name):\n",
    "    try:\n",
    "        # Wikipedia search\n",
    "        search_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"list\": \"search\",\n",
    "            \"srsearch\": name,\n",
    "            \"format\": \"json\"\n",
    "        }\n",
    "        res = requests.get(search_url, params=params)\n",
    "        res.raise_for_status()\n",
    "        results = res.json().get(\"query\", {}).get(\"search\", [])\n",
    "        if not results:\n",
    "            return \"Unknown\"\n",
    "\n",
    "        # Get Wikipedia page\n",
    "        page_title = results[0][\"title\"]\n",
    "        page_url = f\"https://en.wikipedia.org/wiki/{page_title.replace(' ', '_')}\"\n",
    "        soup = BeautifulSoup(requests.get(page_url).text, \"html.parser\")\n",
    "\n",
    "        # First non-empty paragraph\n",
    "        paragraphs = soup.find_all('p')\n",
    "        for p in paragraphs:\n",
    "            if p.text.strip():\n",
    "                first_para = p.text.lower()\n",
    "                break\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "\n",
    "        # Count pronouns\n",
    "        male_count = sum(first_para.count(w) for w in [\" he \", \" his \", \" him \"])\n",
    "        female_count = sum(first_para.count(w) for w in [\" she \", \" her \", \" hers \"])\n",
    "\n",
    "        if male_count > female_count:\n",
    "            return \"Male\"\n",
    "        elif female_count > male_count:\n",
    "            return \"Female\"\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(\"../data/sample_100_movies.csv\")  # change to your file path\n",
    "df[\"director_1_gender\"] = \"\"\n",
    "df[\"director_2_gender\"] = \"\"\n",
    "\n",
    "# Loop through rows\n",
    "for idx, row in df.iterrows():\n",
    "    name1 = row.get(\"director_1_name\")\n",
    "    name2 = row.get(\"director_2_name\")\n",
    "\n",
    "    if pd.notna(name1):\n",
    "        df.at[idx, \"director_1_gender\"] = get_gender_from_wikipedia(name1)\n",
    "        time.sleep(1.5)\n",
    "\n",
    "    if pd.notna(name2):\n",
    "        df.at[idx, \"director_2_gender\"] = get_gender_from_wikipedia(name2)\n",
    "        time.sleep(1.5)\n",
    "\n",
    "# Count \"Unknown\" entries\n",
    "unknown_count = (\n",
    "    (df[\"director_1_gender\"] == \"Unknown\").sum() +\n",
    "    (df[\"director_2_gender\"] == \"Unknown\").sum()\n",
    ")\n",
    "\n",
    "# Save to file\n",
    "df.to_csv(\"../data/supplementary files/sample_100_movies_wikipedia_gender.csv\", index=False)\n",
    "\n",
    "# Final output\n",
    "print(f\"Gender inference complete. {unknown_count} director(s) could not be classified.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d26abc0-10f8-44e3-b9ee-6b6ac2d52ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try to infer gender from dictionary of Indian names\n",
    "## Dictionary found on: https://www.kaggle.com/datasets/shubhamuttam/indian-names-by-gender\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load main dataset and dictionary. Dictionary of Indian names sourced from Kaggle.\n",
    "movies_df = pd.read_csv(\"../data/supplementary files/sample_100_movies_wikipedia_gender.csv\")\n",
    "name_gender_df = pd.read_csv(\"../data/supplementary files/Gender_Data.csv\")\n",
    "name_gender_df['Name'] = name_gender_df['Name'].str.lower()\n",
    "\n",
    "# Gender code → label\n",
    "gender_map = {0: 'Male', 1: 'Female'}\n",
    "\n",
    "# Dictionary: first name → gender(s)\n",
    "name_to_gender = (\n",
    "    name_gender_df.groupby('Name')['Gender']\n",
    "    .apply(lambda g: set(gender_map.get(x, x) for x in g))\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Core gender inference function\n",
    "def infer_gender(name):\n",
    "    if pd.isna(name) or not isinstance(name, str):\n",
    "        return 'Unknown'\n",
    "    first_token = name.split()[0].lower()\n",
    "    genders = name_to_gender.get(first_token)\n",
    "    if genders is None:\n",
    "        return 'Unknown'\n",
    "    if len(genders) == 1:\n",
    "        return list(genders)[0]\n",
    "    return 'Ambiguous'\n",
    "\n",
    "# Wrapper for director_2: return \"\" if name is missing\n",
    "def infer_gender_optional(name):\n",
    "    if pd.isna(name) or not isinstance(name, str) or name.strip() == \"\":\n",
    "        return \"\"\n",
    "    return infer_gender(name)\n",
    "\n",
    "# Apply inference\n",
    "movies_df['director_1_gender'] = movies_df['director_1_name'].apply(infer_gender)\n",
    "movies_df['director_2_gender'] = movies_df['director_2_name'].apply(infer_gender_optional)\n",
    "\n",
    "# Save to 'data/' folder\n",
    "output_path = os.path.join(\"../data/supplementary files/\", \"sample_100_movies_with_dictionary_gender.csv\")\n",
    "movies_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"File saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444be1c4-a475-4961-a052-cfa2abdf350f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upon checking both, it seemed like Wikipedia was able to do an okay job, and the dictionary just needed to fill in some gaps.\n",
    "## At the end of this merge, 13 records remained with an Unknown gender between both director columns.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load both files\n",
    "original_df = pd.read_csv(\"../data/supplementary files/sample_100_movies_with_wikipedia_gender.csv\")\n",
    "new_df = pd.read_csv(\"../data/supplementary files/sample_100_movies_with_dictionary_gender.csv\")\n",
    "\n",
    "# Safety check: ensure matching rows using imdb_id\n",
    "original_df.set_index(\"imdb_id\", inplace=True)\n",
    "new_df.set_index(\"imdb_id\", inplace=True)\n",
    "\n",
    "# List of gender columns to update\n",
    "gender_cols = [\"director_1_gender\", \"director_2_gender\"]\n",
    "\n",
    "for col in gender_cols:\n",
    "    original_values = original_df[col]\n",
    "    new_values = new_df[col]\n",
    "\n",
    "    # Update only where original is \"Unknown\" and new is \"Male\" or \"Female\"\n",
    "    updated_values = original_values.where(\n",
    "        ~((original_values == \"Unknown\") & (new_values.isin([\"Male\", \"Female\"]))),\n",
    "        new_values\n",
    "    )\n",
    "    original_df[col] = updated_values\n",
    "\n",
    "# Reset index and save\n",
    "original_df.reset_index(inplace=True)\n",
    "original_df.to_csv(\"../data/sample_100_movies.csv\", index=False)\n",
    "\n",
    "print(\"Gender columns updated in 'sample_100_movies.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0766751-d8a1-4c37-afa2-3865a4ae7d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GET BOX OFFICE INFORMATION FROM WIKIPEDIA, IF AVAILABLE\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Load movie data\n",
    "df = pd.read_csv(\"../data/sample_100_movies.csv\")\n",
    "\n",
    "# Prepare output folder\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "output_rows = []\n",
    "\n",
    "# Canonical Wikipedia page ID resolver\n",
    "def get_canonical_page_id_and_url(wiki_url):\n",
    "    title = wiki_url.strip().rsplit(\"/\", 1)[-1]\n",
    "    api_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"redirects\": 1\n",
    "    }\n",
    "    r = requests.get(api_url, params=params)\n",
    "    data = r.json()\n",
    "    pages = data[\"query\"][\"pages\"]\n",
    "    page_id = next(iter(pages))\n",
    "    if page_id == \"-1\":\n",
    "        return None, None\n",
    "    canonical_title = pages[page_id][\"title\"].replace(\" \", \"_\")\n",
    "    permalink = f\"https://en.wikipedia.org/wiki/{canonical_title}\"\n",
    "    return int(page_id), permalink\n",
    "\n",
    "# Extract box office from HTML using regex\n",
    "def extract_box_office_from_html(html):\n",
    "    # Limit to infobox section only\n",
    "    infobox_match = re.search(r'(<table class=\"infobox[^>]*>.*?</table>)', html, re.DOTALL)\n",
    "    if not infobox_match:\n",
    "        return None\n",
    "\n",
    "    infobox_html = infobox_match.group(1)\n",
    "\n",
    "    # Find Box office row\n",
    "    match = re.search(r'<th[^>]*>\\s*Box office\\s*</th>\\s*<td[^>]*>(.*?)</td>', infobox_html, re.DOTALL | re.IGNORECASE)\n",
    "    if match:\n",
    "        # Clean value: remove HTML tags, references\n",
    "        raw_text = match.group(1)\n",
    "        cleaned = re.sub(r'<.*?>|\\[.*?\\]', '', raw_text).strip()\n",
    "        return cleaned\n",
    "    return None\n",
    "\n",
    "success, failure = 0, 0\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    imdb_id = row.get(\"imdb_id\")\n",
    "    wiki_url = row.get(\"wiki_link\")\n",
    "\n",
    "    if pd.isna(imdb_id) or pd.isna(wiki_url):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Step 1: Resolve to canonical page\n",
    "        page_id, permalink = get_canonical_page_id_and_url(wiki_url)\n",
    "        if not page_id:\n",
    "            print(f\"IMDb ID {imdb_id}: Page not found\")\n",
    "            failure += 1\n",
    "            continue\n",
    "\n",
    "        # Step 2: Fetch HTML content\n",
    "        html = requests.get(permalink).text\n",
    "\n",
    "        # Step 3: Extract Box Office info\n",
    "        box_office = extract_box_office_from_html(html)\n",
    "\n",
    "        output_rows.append({\n",
    "            \"imdb_id\": imdb_id,\n",
    "            \"wiki_url\": permalink,\n",
    "            \"box_office\": box_office\n",
    "        })\n",
    "\n",
    "        if box_office:\n",
    "            print(f\"{imdb_id} — {box_office}\")\n",
    "            success += 1\n",
    "        else:\n",
    "            print(f\"{imdb_id} — Box office not found\")\n",
    "            failure += 1\n",
    "\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{imdb_id}: {e}\")\n",
    "        failure += 1\n",
    "\n",
    "# Save output\n",
    "output_df = pd.DataFrame(output_rows)\n",
    "output_df.to_csv(\"../data/supplementary files/box_office_from_wikipedia.csv\", index=False)\n",
    "print(f\"\\nDone! Success: {success}, Failures: {failure}\")\n",
    "print(\"Saved: box_office_from_wikipedia.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25349245-167e-4d9e-8e1c-5655c1990d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLEAN UP BOX OFFICE INFORMATION\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import html\n",
    "\n",
    "# Load raw CSV or series\n",
    "df = pd.read_csv(\"../data/supplementary files/box_office_from_wikipedia.csv\")  # or replace with your DataFrame\n",
    "\n",
    "def normalize_box_office(value):\n",
    "    if not isinstance(value, str) or not value.strip():\n",
    "        return None\n",
    "\n",
    "    # Unescape HTML entities and lowercase\n",
    "    text = html.unescape(value.lower())\n",
    "\n",
    "    # Remove text in parentheses and after \"equivalent to\"\n",
    "    text = re.split(r'\\(.*?\\)|equivalent to', text)[0]\n",
    "\n",
    "    # Remove non-breaking spaces, commas, labels\n",
    "    text = text.replace(\"nbsp;\", \"\").replace(\",\", \"\")\n",
    "    text = re.sub(r\"(est\\.?|crores?)\", \"\", text)\n",
    "\n",
    "    # Extract amount + unit\n",
    "    match = re.search(r'₹\\s?([\\d\\.]+)\\s*(crore|million|billion|lakh)?', text)\n",
    "    if not match:\n",
    "        return None\n",
    "\n",
    "    amount = float(match.group(1))\n",
    "    unit = match.group(2)\n",
    "\n",
    "    # Convert to crore\n",
    "    if unit == \"million\":\n",
    "        amount = amount * 0.1  # 1 million = 0.1 crore\n",
    "    elif unit == \"billion\":\n",
    "        amount = amount * 100  # 1 billion = 100 crore\n",
    "    elif unit == \"lakh\":\n",
    "        amount = amount * 0.01  # 1 lakh = 0.01 crore\n",
    "    # if unit is already crore or missing, leave as is\n",
    "\n",
    "    return round(amount, 2)\n",
    "\n",
    "# Apply to box_office column\n",
    "df[\"box_office_cleaned_inr_crore\"] = df[\"box_office\"].apply(normalize_box_office)\n",
    "\n",
    "# Save cleaned file\n",
    "df.to_csv(\"../data/supplementary files/box_office_cleaned.csv\", index=False)\n",
    "print(\"Cleaned values saved to: box_office_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb40194-ab31-459a-8a20-bf6880f40e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CLEANED UP BOX OFFICE DATA TO SAMPLE 100 MOVIES FILE\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "data_folder = os.path.join(\"..\", \"data\")\n",
    "sample_path = os.path.join(data_folder, \"sample_100_movies.csv\")\n",
    "box_office_path = os.path.join(data_folder, \"supplementary files/box_office_cleaned.csv\")\n",
    "\n",
    "# Load the datasets\n",
    "sample_df = pd.read_csv(sample_path)\n",
    "box_office_df = pd.read_csv(box_office_path)\n",
    "\n",
    "# Determine merge key\n",
    "merge_key = \"imdb_id\" if \"imdb_id\" in sample_df.columns and \"imdb_id\" in box_office_df.columns else \"title\"\n",
    "\n",
    "# Merge to add the column (in memory only)\n",
    "sample_df = sample_df.merge(\n",
    "    box_office_df[[merge_key, \"box_office_cleaned_inr_crore\"]],\n",
    "    on=merge_key,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Save in sample_100_movies.csv\n",
    "sample_df.to_csv(sample_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
